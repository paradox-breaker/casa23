# Classification Ⅰ

## Summary

Classification methods include supervised classification and unsupervised classification, distinguished by whether a labeled training dataset is required. Supervised classification aims to assign samples to predefined categories, while unsupervised classification groups samples  based on the similarity of their features, with the meaning of these clusters remaining unclear. This section primarily introduces several supervised classification methods.

### Decision Tree

+---------------------+--------------------------------------------------+-----------------------------------------------------------------------+
| Type                | Important Metrics                                | Construction Process (CART Algorithm：Binary Splits at Each Node)     |
+=====================+==================================================+=======================================================================+
| Classification Tree | Gini Impurity: quantify the mixing of categories | 1.  Compute the Gini impurity for each feature.                       |
|                     | in a node                                        |                                                                       |
|                     |                                                  | 2.  Select the feature with the lowest impurity as the root node.     |
|                     |                                                  |                                                                       |
|                     |                                                  | 3.   Repeat this process to select features at subsequent nodes.      |
+---------------------+--------------------------------------------------+-----------------------------------------------------------------------+
| Regression Tree     | -   SSR (SSR_total = SSR_left + SSR_right)       | 1.  Traverse all features.                                            |
|                     |                                                  |                                                                       |
|                     | -   Tree Score (SSR + α×T)                       | 2.  For each feature, try different split points and compute SSR.     |
|                     |                                                  |                                                                       |
|                     |                                                  | 3.  Select the feature and split point with the smallest SSR.         |
|                     |                                                  |                                                                       |
|                     |                                                  | 4.   Recursively repeat until a stopping condition is met.            |
+---------------------+--------------------------------------------------+-----------------------------------------------------------------------+

: Classification and Regression Trees

If the final trained model has low bias, it may lead to overfitting (a high-variance model), requiring pruning of the decision tree.

```{r echo = FALSE,warning=FALSE}
library(DiagrammeR)

grViz("
digraph flowchart {
  graph [layout = dot, rankdir = TB]

  Step0 [label = 'Build a complete CART regression tree, fitting the training data as much as possible, with the Alpha value set to 0', shape = box]
  Step1 [label = 'Vary Alpha to obtain different Alpha values and their corresponding trees', shape = box]
  Step2 [label = 'Split the data into training and testing sets in a 7:3 ratio', shape = box]
  Step3 [label = 'Train with different Alpha values', shape = box]
  Step4 [label = 'Use the trained trees to predict on the test set', shape = box]
  Step5 [label = 'Repeat the experiment 10 times with different training-test splits', shape = box]
  Step6 [label = 'Identify the Alpha value that minimizes SSR/Gini impurity on the test data', shape = box]
  Step7 [label = 'Select the complete tree corresponding to this Alpha value', shape = box]

  Step0 -> Step1 -> Step2 -> Step3 -> Step4 -> Step5 -> Step6 -> Step7
}
")
```

### Random Forest

RF leverages multiple decision trees simultaneously, randomly selecting a subset of features during both the overall feature selection for the trees and the feature selection for node splitting, to reduce correlation between trees. The final prediction result is based on the majority vote from different trees (for classification) or the average of predictions (for regression).

It should be noted that each tree has its own OOB (Out-of-Bag) samples, which differ from validation data as they are never used for training. OOB samples are mainly used for parameter tuning or model evaluation.

@fig-shenzhen shows the LCC of Shenzhen based on Sentinel-2, with features divided into 4 categories (vegetation, water, bare land, urban). Due to the coarse classification, the overall accuracy of the training set is as high as 99.96%, and that of the test set reaches 99.42%.

![LCC of Shenzhen based on Sentinel-2](image/LCC_shenzhen.png){#fig-shenzhen}

### SVM

SVM is a linear binary classifier that maps data into a high-dimensional feature space to find the hyperplane that maximizes the margin for classification. Its main parameters include the C value and Gamma value: the C value controls the penalty for misclassification, while the Gamma value determines the influence range of each data point. The key focus of SVM is to find a balance between maximizing the margin and minimizing misclassification errors.
