[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Weekly Diary",
    "section": "",
    "text": "Welcome\nThis is my weekly study notes for the CASA0023 course. My undergraduate major was Geographic Information Science, and my thesis focused on vegetation extraction from remote sensing imagery. Therefore, some of my personal reflections may be related to my undergraduate background.\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "wk1.html",
    "href": "wk1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Summary\nRemote sensing is a science that uses electromagnetic radiation as a medium to identify surface features and apply surface information to relevant fields (Navalgund, Jayaraman, and Roy 2007). Various sensors function like human eyes that can perceive a broader range of spectral bands, providing richer and more extensive ground information, thus laying the foundation for further analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "wk1.html#summary",
    "href": "wk1.html#summary",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Active and Passive Remote Sensing\nFigure 1.1 and Table 1.1 demonstate the differences between active and passive remote sensing in terms of working principles, advantages and applications.\n\n\n\n\n\n\nFigure 1.1: Principle Differences Between Active and Passive Remote Sensing. Source:Link\n\n\n\n\n\n\nTable 1.1: Comparison of Active and Passive Remote Sensing\n\n\n\n\n\n\n\n\n\n\nCategory\nActive Remote Sensing\nPassive Remote Sensing\n\n\n\n\nEnergy Source\nSensor-generated energy\nRelies on surface radiation\n\n\nAdvantages\n\nIndependent of surface radiation, unaffected by lighting conditions.\nCan penetrate clouds, vegetation, etc.\n\n\nCan cover large areas simultaneously.\nHigh revisit rate,capable of providing time-series data\n\n\n\nApplications\nSuitable for all-time, all-weather, and extreme environment measurements\nSuitable for large-scale continuous observation\n\n\n\n\n\n\n\n\n1.1.2 Interaction with Earth’s Surface\nAs Figure 1.2 shows, solar radiation undergoes a series of interactions at the Earth’s surface, such as cloud scattering, surface scattering, and atmospheric absorption. The energy ultimately received by the sensor is the result of these combined processes, representing rich information while also potentially introducing interference to the research. For example, atmospheric correction is required in the preprocessing of remote sensing images because the energy received by the sensor is affected by atmospheric scattering and absorption, leading to reduced image contrast.\n\n\n\n\n\n\nFigure 1.2: Interaction with Earth’s Surface. Source: Link\n\n\n\n\n\n1.1.3 Four Types of Resolution\nTypically, the design of remote sensing sensors requires a trade-off between these four types of resolution based on their primary application areas. For example, high spatial resolution often comes at the cost of a smaller coverage area, which results in a longer revisit period, meaning lower temporal resolution. Additionally, due to technical constraints such as data transmission, spatial resolution and spectral resolution cannot be simultaneously maximized.\n\nDefinitions of the Four Types of Resolution in Remote Sensing\n\n\n\n\n\n\nResolution Type\n Definition\n\n\n\n\nSpatial Resolution\nThe smallest ground unit distinguishable by the sensor, which corresponds to the size of a raster pixel.\n\n\nSpectral Resolution\nThe number and width of spectral bands that the sensor can detect.\n\n\nTemporal Resolution\nThe revisit cycle of the sensor.\n\n\nRadiometric Resolution\nThe smallest detectable energy variation by the sensor.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "wk1.html#application",
    "href": "wk1.html#application",
    "title": "1  Introduction",
    "section": "1.2 Application",
    "text": "1.2 Application\nRemote sensing has been widely studied in various fields, including agricultural management, environmental monitoring, geological exploration, climate change research, land planning, and disaster emergency response. The applicability of active and passive remote sensing varies across different research areas. For example, in terrain mapping, passive remote sensing lacks strong penetration capability, making it difficult to acquire exposed terrain information. In contrast, active remote sensing techniques such as LiDAR and SAR can penetrate surface cover and directly measure elevation. Similarly, in vegetation analysis, the spectral characteristics of vegetation align well with solar radiation, allowing passive remote sensing to provide rich spectral information. This makes it easier to calculate vegetation indices and efficiently monitor vegetation changes.\nIn addition, different fields have varying requirements for image resolution. For instance, urban planning requires high spatial resolution to identify fine structures such as roads and buildings. Weather monitoring demands frequent data updates to capture rapidly changing atmospheric conditions. Vegetation analysis relies on abundant spectral information to detect subtle variations in vegetation health. Temperature monitoring requires sensors with high radiometric resolution to capture small temperature differences accurately.\nTaking the application of remote sensing in agriculture as an example, the technology primarily utilized in this field is passive remote sensing, which aims to obtain information about plant growth and health conditions. Remote sensing in agriculture can be categorized into several aspects, including crop identification, growth monitoring, yield estimation, pest and disease control, and irrigation management, all of which help optimize agricultural operations and enhance production efficiency.\nThe advancement of remote sensing technology, particularly improvements in satellite image resolution, has significantly contributed to the development of precision agriculture. For instance, high spatial resolution data provides more detailed ground information, aiding in land cover identification and soil condition monitoring. Meanwhile, hyperspectral imagery captures finer spectral details, such as the quantification of solar-induced chlorophyll fluorescence, which helps estimate photosynthetic activity, reflecting crop nutritional status and stress response capabilities (Sishodia, Ray, and Singh 2020).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "wk1.html#reflection",
    "href": "wk1.html#reflection",
    "title": "1  Introduction",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\n\n1.3.1 Understanding Remote Sensing Sensors\nI find it fascinating to compare the information received by the human eye with that received by remote sensing sensors. Despite experiencing the same Rayleigh scattering, observers at different locations may see different sky colors due to variations in the atmospheric path length of sunlight. Similarly, remote sensing satellites are highly susceptible to atmospheric effects, highlighting the importance of atmospheric correction. However, unlike the three types of cone cells in the human eye that perceive color and detail, remote sensing sensors are not limited to three spectral bands. For instance, the combination of the near-infrared and red bands can effectively distinguish vegetation, presenting a standard false-color representation that differs from true-color imagery.\n\n\n1.3.2 Methods for Balancing the Four Resolutions\nAs mentioned earlier, it is challenging to simultaneously optimize all four types of resolution in a single task. Therefore, in practical applications, multi-source data fusion is often required. For example, to balance spatial and spectral resolution, high-spectral but low-spatial-resolution imagery can be fused with high-spatial-resolution imagery. Additionally, temporal interpolation can be used to integrate high-temporal, low-spatial-resolution data (e.g., MODIS) with low-temporal, high-spatial-resolution data (e.g., Sentinel-2) to generate imagery that balances both temporal and spatial resolution.\n\n\n1.3.3 Future Prospects of Remote Sensing Technology\nAt present, remote sensing technology is evolving toward higher resolution, greater intelligence, and improved efficiency. Enhancing sensor performance while integrating artificial intelligence can significantly improve data processing efficiency and automation. Taking land cover classification as an example, the introduction of machine learning has enhanced classification accuracy and stability. In the future, the development of adaptive classification methods that reduce the need for manual labeling could further enhance the automation and efficiency of remote sensing classification.\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "wk2.html",
    "href": "wk2.html",
    "title": "2  Presentation",
    "section": "",
    "text": "These are the introduction slides for the MSI sensor onboard the Sentinel-2 satellite series.\n\n\n\n\n\n\n\n\nLink\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Presentation</span>"
    ]
  },
  {
    "objectID": "wk3.html",
    "href": "wk3.html",
    "title": "3  Data Correction and Enhancement",
    "section": "",
    "text": "3.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Correction and Enhancement</span>"
    ]
  },
  {
    "objectID": "wk3.html#summary",
    "href": "wk3.html#summary",
    "title": "3  Data Correction and Enhancement",
    "section": "",
    "text": "3.1.1 Data Organization\n\n3.1.1.1 Format Conversion\nIf specific software is required for image analysis, it is necessary to select a format compatible with that software.\n\n\n3.1.1.2 Image Mosaicking\nSince single images usually cover limited areas, and sometimes a study area spans across two adjacent scenes, it is necessary to mosaic two images.\n\n\n\n\n\nfun=“mean” means calculating the average value in overlapping areas.\nThis section selects an area near the city of Huzhou, China. The final true-color image after mosaicking has a smooth transition without noticeable seams.\n\n\n\n\n\nSometimes, when the pixel values in the overlapping areas of two images differ significantly, the seam may become prominent. In such cases, feathering techniques need to be applied.\n\n\n3.1.1.3 Image Clipping\nSince the coverage of remote sensing images is large, processing and analyzing the entire image may result in unnecessarily computation. Therefore, image clipping is necessary.\nFor example, if I want to classify land cover in Anji County, I can use the boundary vector data to clip the image.\n\n\n\nLandsat 9 True-Color Remote Sensing Image of Anji County\n\n\nI can also determine a threshold based on NDVI values to extract vegetation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.2 Data Correction\n\n3.1.2.1 Radiometric Calibration\nSince the sensor receives digital signals, it is necessary to convert them to radiometric units with physical significance, such as radiance. The gain and bias can be obtained from the sensor metadata.\n\n\n\n3.1.2.2 Geometric Correction\nGeometric correction mainly consists of two parts:\n\nEstablishing a mapping relationship between the image to be corrected and the reference image based on selected control points.\nResampling the image.\n\nThe choice of resampling method depends on the specific application. For example, nearest neighbor resampling is suitable for classification tasks, while cubic interpolation is more appropriate for image analysis.\n\n\n3.1.2.3 Atmospheric Correction\nAtmospheric correction is the process of removing atmospheric effects to restore surface reflectance. The necessity of atmospheric correction depends on the specific application. For example:\n\nLand cover classification does not necessarily require atmospheric correction.\nQuantitative parameter extraction requires it for accurate results.\n\n\nAbsolute Calibration VS Relative Calibration\n\n\n\n\n\n\n\n\nAbsolute Calibration\nRelative Calibration\n\n\n\n\nObjective\nConvert digital brightness values into scaled surface reflectance.\nNormalize radiometric intensities across different images and bands.\n\n\nMethods\n\nAtmospheric transfer models\nEmpirical Line Calibration\n\n\nDark Object Subtraction\nHistogram Adjustment\nPseudoinvariant Features\n\n\n\nTypical Applications\nTypical Applications\nTime-series analysis.\n\n\n\n\n\n\n3.1.3 Image Enhancement\nImage enhancement refers to the process of improving image quality and highlighting key information using processing techniques. It can be used for smoothing and denoising, edge detection, and contrast enhancement, making images more suitable for observation and analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Correction and Enhancement</span>"
    ]
  },
  {
    "objectID": "wk3.html#application",
    "href": "wk3.html#application",
    "title": "3  Data Correction and Enhancement",
    "section": "3.2 Application",
    "text": "3.2 Application\nRadiometric calibration, geometric correction and atmospheric correction fall within the scope of remote sensing data preprocessing. The flowchart below intuitively illustrates the position of these three steps in practical remote sensing data applications.\n\n\n\n\n\n\n\n\n\nCurrently, many fields that use remote sensing data rely on Level-2 (L2) data because it has already undergone basic radiometric calibration and atmospheric correction, making it ready for direct analysis. For example, in multi-temporal analyses such as land use change and vegetation cover monitoring, standardized Level-2 data eliminates atmospheric interference, enhancing comparability and reducing the need for user preprocessing.\nOf course, not all research requires preprocessed data. For instance, some studies focus on atmospheric analysis or the development and testing of new correction algorithms, in which case more raw data would be more suitable.\nCompared to image correction, the application of image enhancement is relatively more concentrated, primarily focusing on visual enhancement and improving readability. For example, when using ENVI software for land cover classification, if the acquired image has a narrow spectral range that affects manual interpretation, stretching is needed to facilitate land cover recognition and build a reasonable training set. It is important to note that image stretching is mainly used to enhance details and typically does not alter the original pixel values. However, if the enhanced image is to be applied in further analysis, more caution is required, as the enhancement method should improve image contrast and edge information while preserving the original reflectance values (Kaplan, Erer, and Kumlu 2021).\nRamzan et al. (2025) fused Landsat 8 and Landsat 9 imagery to enhance the accuracy of crop classification and applied image enhancement after fusion to emphasize texture variations and spectral differences. Clearly, this step is crucial for precise classification, as it highlights key features and improves subsequent classification accuracy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Correction and Enhancement</span>"
    ]
  },
  {
    "objectID": "wk3.html#reflection",
    "href": "wk3.html#reflection",
    "title": "3  Data Correction and Enhancement",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\n\n3.3.1 Real-time Processing\nSome application areas, such as emergency response in natural disasters, have high real-time requirements. It is crucial to rapidly acquire and analyze image data of affected areas to improve response efficiency. The widespread adoption of UAVs provides a continuous data input source for real-time processing.\nCurrently, research has demonstrated the feasibility of implementing edge computing on UAVs to directly perform classification tasks, enabling real-time data monitoring. This approach is also applicable to real-time image correction and enhancement, facilitating the efficient execution of subsequent tasks (Hernández et al. 2022). However, this field still faces challenges related to cost and computational overhead. Therefore, further research is needed to optimize algorithms and improve model accuracy.\n\n\n3.3.2 Adaptability and Generality\nThe correction methods for different sensors are not the same. In geometric correction, due to variations in imaging modes among sensors, optical sensors primarily use ground control points (GCPs) and sometimes utilize digital elevation models (DEM) for orthorectification. In contrast, synthetic aperture radar (SAR), due to its side-looking imaging geometry, requires range-Doppler geometric correction.\nFor atmospheric correction, Sentinel-2 employs the 6S model, while hyperspectral data requires band-by-band correction because the large number of spectral bands leads to a greater impact of atmospheric absorption.\nTo maintain high efficiency in dynamic environments or to standardize the correction process for multiple sensors mounted on the same platform, a generalized framework and adaptive modules are needed. With advancements in machine learning and AI algorithms, improvements in adaptability have become possible. However, practical limitations remain, such as the conflict between adaptability and generality—excessive pursuit of generality may lead to insufficient optimization for individual sensors, while emphasizing adaptability too much can increase computational burden.\nOne emerging approach to address this challenge is developing a universal core algorithm while allowing customization through plugins or configuration files, enabling tailored correction methods for different sensors.\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Correction and Enhancement</span>"
    ]
  },
  {
    "objectID": "wk6.html",
    "href": "wk6.html",
    "title": "4  Google Earth Engine",
    "section": "",
    "text": "4.1 Summary\nThe week mainly focuses on Practical examples to analyze functions and operations that may be involved in remote sensing data processing using GEE.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "wk6.html#summary",
    "href": "wk6.html#summary",
    "title": "4  Google Earth Engine",
    "section": "",
    "text": "4.1.1 Loading Data\nIn this case, we select Landsat 8 Collection 2, T1, L2 data and filter it by time and region, using lt (less than) to select images with cloud cover below 10%.\n\nAfter printing, the console shows the image count and their path-row numbers.\n\n\n\n4.1.2 Radiometric Calibration\nUse regular expressions to filter images.\nDifferent scaling factors are applied to reflectance and brightness temperature.\n“True” indicates that the original image bands will be replaced.\n\n\n\n4.1.3 Visualizing Images\nHere, the median of multiple images is computed.\nmin and max control image stretching:\nPixels with values ≤ min (≥ max) are mapped to the darkest (brightest) color.\nValues in between are linearly stretched.\n\n\nThe image is modified to standard false color in the layer settings.\n\n\n\n4.1.4 Mosaic images\nThe mosaic effect here is not ideal, as there is a noticeable stacking effect.\n\n\n\n4.1.5 Clip images\n\n\n\n4.1.6 Texture measures\nSince the reflectance values are small, but .glcmTexture() requires integer values, the data needs to be stretched.\n\nHere, the values are multiplied by 1000.\n{size: 1} specifies a 3×3 window is used.\nSelected texture metrics: contrast and dissimilarity.\n.toUint16() converts the data to 16-bit integers, as GLCM calculations cannot process 32-bit floating-point numbers.\n\n\n\n\n4.1.7 PCA\nPerform some preliminary setup.\nmeanDict.values(bandNames) extracts the mean of each band as a constant image (without spatial variation) for mean centering.\n\nThen, start defining the function.\nConvert the image so that each pixel contains an array storing multi-band values. The default image storage format is not suitable for direct matrix computations.\n\ncovar is an ee.Dictionary containing the covariance matrix and get(‘array’) retrieves it.\ncovarArray.eigen() computes the eigenvalues and eigenvectors.\nThe result is then sliced using eigens.slice(1, 0, 1) (Dimension = 1 (slicing by rows);Start from row 0, excluding subsequent rows)\nConverted into a 1D list, from which the total variance is calculated.\nFinally, the variance contribution rate of each component is computed.\n\ntoArray(1) converts the 1D array into a 2D array (row-wise), making it suitable for matrixMultiply() computations.\n.arrayFlatten([…]) converts the array image into a regular image format, with a list defining the new band names.\nFinally, extra dimensions in PrincipalComponents are removed, bands are renamed, and normalization is applied.\n\nUltimately, the first four principal components explain more than 99.5% of the variance.\n\n\n\n4.1.8 NDVI",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "wk6.html#application",
    "href": "wk6.html#application",
    "title": "4  Google Earth Engine",
    "section": "4.2 Application",
    "text": "4.2 Application\nGEE is a platform for large-scale geospatial data processing and has a wide range of applications across multiple fields, including resource management, environmental monitoring, land use analysis, disaster response, and urban planning. Sidhu, Pebesma, and Câmara (2018) utilized GEE to analyze land use changes in small terrestrial areas of Singapore by generating time series plots of the Enhanced Vegetation Index (EVI). Their study compared the effectiveness of EVI time series products derived from different data sources (MODIS and Landsat), analyzed the causes of EVI value variations, and discussed both the advantages of GEE and the challenges faced when conducting time series analysis on the platform.\nThe study results revealed that:\n\nThe Tuas industrial area has undergone rapid industrialization since 2006, primarily relying on land reclamation techniques for development. In contrast, the forest cover in the Central Catchment Area has remained unaffected by human activities, with EVI fluctuations aligning with the Southeast Asian monsoon cycle, indicating that vegetation changes are primarily driven by monsoonal variations.\nMODIS, while having low spatial resolution, offers high temporal resolution, enabling frequent data acquisition. This makes it effective in capturing subtle land cover changes and seasonal trends. On the other hand, Landsat, despite its high spatial resolution, has poor temporal resolution, making it difficult to track complete change dynamics. Additionally, some dates suffer from data loss due to severe cloud cover.\nThanks to its MapReduce architecture, GEE demonstrates excellent spatial computing capabilities and can rapidly process imagery on a global scale. However, this very architecture also imposes limitations when simultaneously supporting both spatial and temporal analyses. During the Reduce phase—where data from different time points are integrated—a significant amount of data transmission occurs, increasing computational overhead. As a result, even for small terrestrial areas, processing data beyond a certain time range can lead to computation timeouts.\n\n\n\n\nIncrease in Computation Time Until Timeout",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "wk6.html#reflection",
    "href": "wk6.html#reflection",
    "title": "4  Google Earth Engine",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nThrough the hands-on practice and literature review, it becomes evident that GEE offers numerous significant advantages, such as:\n\nBuilt-in access to extensive remote sensing datasets: Users can directly access high-quality remote sensing data, such as Sentinel and Landsat, from the cloud without the need for time-consuming manual downloads and preprocessing. This significantly reduces both time costs and computational storage costs.\nNo need to manually set up a cloud computing environment: Unlike platforms such as GeoTrellis and SciDB, which require users to deploy their own servers or databases, GEE eliminates the need for any computational environment setup. This provides a major advantage for non-specialist users, allowing them to leverage cloud computing without having to learn complex big data processing tools, thus facilitating interdisciplinary research collaboration.\nProvides APIs for direct large-scale data processing: GEE employs a parallel computing architecture that efficiently processes large-scale data. In spatial computations, GEE enables simultaneous calculations for all pixels rather than processing them individually. Additionally, GEE automatically allocates computing resources to optimize task execution. The API is also designed to be simple and efficient, enabling users to perform complex remote sensing analyses with minimal code effort.\n\nIt can be said that GEE is a truly research-friendly and efficient platform, backed by Google’s technical support, ensuring continuous optimization and long-term development potential.\nHowever, as mentioned earlier, GEE has limitations in time-series analysis. In the future, improvements to the MapReduce computing architecture could further enhance its ability to handle time-series analysis, reducing issues such as computational timeouts and improving overall processing efficiency.\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Google Earth Engine</span>"
    ]
  },
  {
    "objectID": "wk7.html",
    "href": "wk7.html",
    "title": "5  Classification Ⅰ",
    "section": "",
    "text": "5.1 Summary\nClassification methods include supervised classification and unsupervised classification, distinguished by whether a labeled training dataset is required. Supervised classification aims to assign samples to predefined categories, while unsupervised classification groups samples  based on the similarity of their features, with the meaning of these clusters remaining unclear. This section primarily introduces several supervised classification methods.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "wk7.html#summary",
    "href": "wk7.html#summary",
    "title": "5  Classification Ⅰ",
    "section": "",
    "text": "5.1.1 Decision Tree\n\nClassification and Regression Trees\n\n\n\n\n\n\n\nType\nImportant Metrics\nConstruction Process (CART Algorithm：Binary Splits at Each Node)\n\n\n\n\nClassification Tree\nGini Impurity: quantify the mixing of categories in a node\n\nCompute the Gini impurity for each feature.\nSelect the feature with the lowest impurity as the root node.\nRepeat this process to select features at subsequent nodes.\n\n\n\nRegression Tree\n\nSSR (SSR_total = SSR_left + SSR_right)\nTree Score (SSR + α×T)\n\n\nTraverse all features.\nFor each feature, try different split points and compute SSR.\nSelect the feature and split point with the smallest SSR.\nRecursively repeat until a stopping condition is met.\n\n\n\n\nIf the final trained model has low bias, it may lead to overfitting (a high-variance model), requiring pruning of the decision tree.\n\n\n\n\n\n\n\n\n5.1.2 Random Forest\nRF leverages multiple decision trees simultaneously, randomly selecting a subset of features during both the overall feature selection for the trees and the feature selection for node splitting, to reduce correlation between trees. The final prediction result is based on the majority vote from different trees (for classification) or the average of predictions (for regression).\nIt should be noted that each tree has its own OOB (Out-of-Bag) samples, which differ from test data as they are never used for training. OOB samples are mainly used for parameter tuning or model evaluation.\nFigure 5.1 shows the LCC of Shenzhen based on Sentinel-2, with features divided into 4 categories (vegetation, water, bare land, urban). Due to the coarse classification, the overall accuracy of the training set is as high as 99.96%, and that of the test set reaches 99.42%.\n\n\n\n\n\n\nFigure 5.1: LCC of Shenzhen based on Sentinel-2\n\n\n\n\n\n5.1.3 SVM\nSVM is a linear binary classifier that maps data into a high-dimensional feature space to find the hyperplane that maximizes the margin for classification. Its main parameters include the C value and Gamma value: the C value controls the penalty for misclassification, while the Gamma value determines the influence range of each data point. The key focus of SVM is to find a balance between maximizing the margin and minimizing misclassification errors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "wk7.html#application",
    "href": "wk7.html#application",
    "title": "5  Classification Ⅰ",
    "section": "5.2 Application",
    "text": "5.2 Application\nThere are numerous application areas for land cover classification using remote sensing imagery, These include, but are not limited to crop growth monitoring and yield prediction, urban land use planning, resource management, and environmental change monitoring.\nIn the agricultural domain, remote sensing products with red-edge bands are often preferred, as this spectral region is highly sensitive to changes in chlorophyll content，which directly reflects plant health and development. For example, Valero Medina and Alzate Atehortúa (2019) used RapidEye satellite imagery to distinguish the growth stages of cotton in San Pelayo, a traditional cotton-growing region in Colombia. They compared the performance of three classification algorithms—Maximum Likelihood, Random Forest, and Support Vector Machine (SVM)—and found that SVM performed best in terms of classification accuracy and preservation of geometric detail. Such techniques can be applied to monitor crop growth conditions and development stages, providing valuable support for agricultural management.\nFor environmental change monitoring, which often requires long-term time series data, Landsat imagery is widely used due to its free accessibility and broad temporal coverage. For instance, coastal wetlands are of high research interest due to their ecological functions such as climate regulation, windbreak and sand fixation, and provision of habitats. By using multi-temporal Landsat imagery and referring to internationally recognized land cover classification standards, researchers can classify coastal land cover types and identify the spatial distribution and dynamic changes of various features (e.g., urban expansion, coastal erosion). This helps assess the degree of degradation and provides a scientific basis for the formulation and implementation of ecological restoration policies (Lemenkova 2024).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "wk7.html#reflection",
    "href": "wk7.html#reflection",
    "title": "5  Classification Ⅰ",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nThe history of land cover classification based on remote sensing data can be traced back to the early 1970s, following the launch of Landsat-1. Since then, classification methods have evolved rapidly—from traditional approaches such as Maximum Likelihood Classification and Minimum Distance Classification to the advanced machine learning and deep learning techniques used today.\nAt present, Random Forest (RF) and Support Vector Machine (SVM) have become two of the most widely adopted machine learning methods in remote sensing classification. Each has its own strengths and limitations and is suited to different research contexts. One reason for SVM’s strong performance in tasks like distinguishing the growth stages of cotton, as mentioned above, may be its ability to project input data into a high-dimensional feature space, making it particularly sensitive to subtle spectral differences. However, this advantage comes at a cost: SVM is highly sensitive to parameter tuning, and inappropriate parameter settings can negatively affect classification accuracy. Moreover, due to its reliance on high-dimensional matrix operations, SVM struggles with scalability and is less efficient when applied to large-scale land cover classification tasks.\nIn contrast, Random Forest, by employing an ensemble of decision trees and a majority-voting mechanism, exhibits strong robustness to noise and requires relatively little data preprocessing (Adugna, Xu, and Fan 2022). Nevertheless, this ensemble approach may also smooth over finer spectral variations, potentially overlooking subtle differences between classes.\nIn recent years, deep learning methods—such as Convolutional Neural Networks (CNNs)—have been widely applied in land cover classification. With the continued development of classification techniques, it is anticipated that more diverse and precise tools will become available, enabling better adaptation to the specific requirements of various research applications.\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Classification Ⅰ</span>"
    ]
  },
  {
    "objectID": "wk8.html",
    "href": "wk8.html",
    "title": "6  Classification Ⅱ",
    "section": "",
    "text": "6.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "wk8.html#summary",
    "href": "wk8.html#summary",
    "title": "6  Classification Ⅱ",
    "section": "",
    "text": "6.1.1 Object-Based Image Analysis\nObject-Based Image Analysis divides an image into small regions—called superpixels—based on the similarity of pixel characteristics such as color, texture, and spatial location, and then performs classification. The use of superpixels can reduce computational load and enhance the preservation of spatial structural information.\nCurrently, the SLIC algorithm is one of the most commonly used methods for generating superpixels.\n\n\n\n\n\n\nIt is important to note that OBIA is less suitable for images with a high proportion of mixed pixels or unclear geometric structure.\n\n\n6.1.2 Sub-pixel Analysis\nThis method assumes that the reflectance of each pixel is a linear combination of the reflectance values of several spectral endmembers—the pure spectral signatures of single land cover types.\nIn applications, most pixels are mixed. If too many endmembers are selected, the computational cost becomes high; if too few are used, the representation of land cover types may be inaccurate.\nFigure 6.1 shows the result of sub-pixel analysis of Dar-es-salaam based on Landsat 8 imagery. The endmembers include urban, vegetation, and water. Since only three endmembers were selected, the categories can be visually distinguished by directly mapping them to RGB channels.\n\n\n\n\n\n\nFigure 6.1: Sub-pixel Analysis of Dar-es-salaam Based on Landsat 8 Imagery\n\n\n\nFigure 6.2 shows the classified image obtained by filtering based on the proportion of each endmember.\n\n\n\n\n\n\nFigure 6.2: Classification Based on Sub-pixel Analysis\n\n\n\n\n\n6.1.3 Accuracy Assessment\n\n6.1.3.1 Confusion Matrix\n\n\n\nConfusion Matrix\n\n\n\n\n6.1.3.2 Kappa\nKappa focuses on whether the model’s predictive performance is better than random guessing, making it a stricter metric than OA. However, when class imbalance exists, Kappa may easily give a more pessimistic evaluation than the model’s actual performance.\n\n\n\n6.1.4 Model Performance Evaluation\n\n6.1.4.1 Discriminative Ability (AUROC)\nThe ROC curve illustrates the trade-off between TPR and FPR as the classification threshold changes. Typically, increasing TPR also leads to an increase in FPR.\nAUROC is the area under the ROC curve. In general, a model’s ROC curve lies between the diagonal line (AUROC = 0.5, representing random guessing) and the ideal curve (AUROC = 1).\n\n\n6.1.4.2 Generalization Performance (Cross Validation)\nK-Fold Cross Validation: In each iteration, 1/k of the dataset is used as the test set, and the remaining (k−1)/k is used for training. This is repeated k times.\nLeave-One-Out Cross Validation is a special case of K-Fold where k equals the total number of samples, making it suitable for small datasets.\n\n6.1.4.2.1 Spatial Cross Validation\nBefore cross-validation, the data is first clustered based on spatial proximity to form folds. This ensures that the training and test sets are spatially separated, avoiding “spatial cheating”.\n\n\n6.1.4.2.2 Example (SVM)\n\n\n\nNested Cross Validation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "wk8.html#application",
    "href": "wk8.html#application",
    "title": "6  Classification Ⅱ",
    "section": "6.2 Application",
    "text": "6.2 Application\n\n6.2.1 Object-Based Classification\nIn practical applications, object-based land cover classification is more suitable for high-resolution imagery and study areas with significant human activity, such as urban environments and farmland. This is because high-resolution imagery often reveals clearer geometric structures of land features, and human activities tend to create distinct boundaries. In contrast, mixed pixels caused by low spatial resolution and blurred boundaries due to interlaced vegetation growth can make image segmentation and classification more difficult.\nThere is abundant literature demonstrating the accuracy of object-based land cover classification in urban studies. For example, Kutz, Cook, and Linderman (2022) used high-resolution remote sensing data to perform land cover classification and change detection in the Baltimore area. The study showed that land cover maps generated using object-based classification for the years 1999 and 2004 achieved high accuracy. Moreover, compared to pixel-based methods, the object-based approach performed better in detecting land cover changes.\nHowever, this does not mean that object-based land cover classification lacks value in natural landscape classification. Certain algorithms can be used to assist in superpixel segmentation. For instance, image texture features extracted using Gabor filters can be employed to improve the accuracy of segmentation methods (Kutz, Cook, and Linderman 2022).\n\n\n6.2.2 Sub-pixel Analysis\nSub-pixel analysis can be used to improve classification accuracy in heterogeneous areas. It can also estimate the proportional coverage of different land cover types within each pixel, enabling the calculation of land cover composition over larger spatial scales to support environmental management and urban planning.\nFor example, Asner and Heidebrecht (2005) used sub-pixel analysis based on hyperspectral data to estimate the surface proportions of photosynthetic vegetation, non-photosynthetic vegetation, and bare soil in northern Chihuahua. This was done to assess the impacts of desertification on plant growth, ecosystem processes, and the carbon cycle. Clearly, a greater number of spectral bands can effectively reduce spectral overlap between different surface components, thereby improving the accuracy of the analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "wk8.html#reflection",
    "href": "wk8.html#reflection",
    "title": "6  Classification Ⅱ",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nAt present, research on land cover classification based on remote sensing imagery generally follows a relatively fixed workflow:\nremote sensing imagery → preprocessing → selection of training samples and classifier → model training → accuracy assessment.\nBy the end of this week, we have developed a full understanding of this process.\nThis week, our focus was mainly on accuracy assessment, and it became clear that different evaluation metrics emphasize different aspects. Certain application fields place greater importance on Producer’s Accuracy (PA)—for instance, in flood monitoring, it is crucial to identify as many true instances as possible, as missed detections can lead to severe consequences. Other applications prioritize User’s Accuracy (UA)—such as mineral resource identification, where it is essential to minimize false positives to avoid excessive human, financial, and material investments in ordinary rock areas.\nAlmost all land cover classification studies report Overall Accuracy (OA), but OA does not always reflect the true performance of the classification. For example, when using remote sensing imagery to extract cotton planting areas—dividing land cover into “cotton planting” and “non-cotton planting”—the sample size between these two classes may be highly imbalanced. Even if the model performs poorly in identifying cotton planting areas, it may still achieve a seemingly high OA by correctly classifying a large number of non-cotton areas.\nTherefore, in practical research, it is necessary to identify and address such potential issues. At the data level, oversampling of minority classes can be used to direct more attention to them. At the model level, hierarchical classification methods can be applied. In the accuracy assessment phase, using multiple evaluation metrics can help compensate for the limitations of any single metric. Moreover, by combining the outcomes and characteristics of these metrics, the classification performance can be discussed in greater depth.\nIn recent years, several new evaluation indicators have been proposed, such as cost-sensitive weighted metrics and temporal consistency indicators. With the advancement of multi-source data and classification algorithms, it is likely that more diverse evaluation methods will be adopted in the future to meet the needs of different classification tasks.\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Classification Ⅱ</span>"
    ]
  },
  {
    "objectID": "wk9.html",
    "href": "wk9.html",
    "title": "7  SAR",
    "section": "",
    "text": "7.1 Summary\nSAR observes Earth’s surface by receiving backscattered signals. It is called “synthetic aperture” radar because, as it moves, it collects data from different positions over the same ground area. Subtle differences in the phase and intensity of these multiple observations allow for precise reconstruction of image details, thereby improving spatial resolution—essentially simulating a much longer antenna.\nRadar signals respond differently to various surface types depending on the polarization mode. For surface roughness analysis, VV polarization is typically used. For detecting volume scattering from structurally loose objects like tree canopies, VH or HV polarization is more appropriate.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SAR</span>"
    ]
  },
  {
    "objectID": "wk9.html#summary",
    "href": "wk9.html#summary",
    "title": "7  SAR",
    "section": "",
    "text": "7.1.1 SAR Values\n\n\n\n\n\n\n\n\nScale\nDefinition\nApplication\n\n\n\n\nPower scale\nOriginal radar signal strength\n\nSuitable for classification, change detection, etc.\nNot ideal for visualization\n\n\n\nAmplitude scale\nSquare root of power\nSuitable for visualization\n\n\ndB scale\n\n\nSuitable for distinguishing dark areas\nNot ideal for quantitative analysis\n\n\n\n\n\n\n7.1.2 Change Detection\nROC curves can be used to identify which comparison method is most suitable for the current study.\n\n\n\n\n\n\n\n\nCategory\nFormula\nMeaning\n\n\n\n\n(Original) Ratio Images\n\nCompares two images from different periods\n\n\nImproved Ratio\n\n\nAn improved version of the original ratio image;\nAvoids division by zero and applies normalization\n\n\n\nMean Ratio Images\n\nApplies the ratio to neighborhood means of pixels to reduce the impact of outliers\n\n\nLog Ratio Images\n\nConverts multiplicative noise into additive noise in the log domain\n\n\nImproved Ratio Log Ratio Images\n\nCombines normalization with logarithmic transformation for better visual and statistical balance\n\n\n\n\n\n7.1.3 Change Detection from Image Collections\n\n7.1.3.1 Statistical Testing\nA t-test can be applied.\nFigure 7.1 shows t-test results of SAR data before and after the Beirut port explosion. Figure 7.2 shows a control test with the date shifted to 2019, noticeably fewer regions show significant change (yellow), indicating that much of the change observed in Figure 7.1 is due to the explosion.\n\n\n\n\n\n\nFigure 7.1: Pixelwise T-Test, 2020\n\n\n\n\n\n\n\n\n\nFigure 7.2: Pixelwise T-Test, 2019\n\n\n\n\n\n7.1.3.2 Standard Deviation\nChanged pixels will typically exhibit higher standard deviation over a certain period. By setting a threshold, it is possible to identify changed pixels.\n\n\n7.1.3.3 Image Fusion\n\n\n\n\n\n\n\nLevel\nExplanation\n\n\n\n\nDecision level\nRadar and optical data are treated independently, as if each occupies its own “band”.\n\n\nObject level\nRadar and optical data are combined as separate features in the object’s feature space\n\n\nImage level\nNew pixel values are calculated, methods include weighting, PCA, and band substitution.\nThe IHS fusion method is a form of band substitution: It converts an optical RGB image to IHS, replaces the intensity component with SAR data, and converts back to RGB.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SAR</span>"
    ]
  },
  {
    "objectID": "wk9.html#application",
    "href": "wk9.html#application",
    "title": "7  SAR",
    "section": "7.2 Application",
    "text": "7.2 Application\nDue to its all-weather, all-day imaging capability and a certain degree of penetration, SAR is widely used for monitoring surface changes and has found broad applications across various fields, such as disaster management, infrastructure monitoring, agricultural management, ocean observation, archaeology, and heritage conservation.\n\n7.2.1 Ocean Observation\nIn the field of ocean monitoring, SAR can be used to measure water depth. By analyzing the amplitude and phase of the backscattered signals, SAR is capable of monitoring ocean currents and wave patterns, and further deriving wind speed and direction over the sea surface.\nIn addition, SAR is highly sensitive to the reflective characteristics of oil films on the ocean surface, making it useful for oil spill detection. This contributes to timely response in oil pollution incidents, helps maintain marine ecosystem health, and protects the well-being of coastal populations.\nMoreover, SAR can effectively detect ships under various weather conditions, which plays a critical role in combating illegal fishing, smuggling, and other unlawful maritime activities. It enhances the safety of maritime navigation and supports the sustainable management of marine resources (Asiyabi et al. 2023).\n\n\n7.2.2 Agricultural Management\nIn the field of agricultural management, Sonobe (2019) utilized VH and VV polarized SAR data acquired by ASNARO-2 (X-band) and Sentinel-1 (C-band) to classify crops in Japan’s Tokachi Plain, aiming to explore the potential of such data combinations for crop mapping. The study evaluated classification results using ASNARO-2 XSAR, Sentinel-1 C-SAR, and a combination of both data sources.\nThe results showed that XSAR data performed better in identifying sugar beet and potato fields, while C-SAR was more suitable for detecting large wheat fields. The complementary use of both datasets helped reduce misclassifications.\nThe study also revealed that different polarization modes have varying penetration capabilities depending on the crop type, suggesting that polarimetric analysis may be useful for future crop identification.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SAR</span>"
    ]
  },
  {
    "objectID": "wk9.html#reflection",
    "href": "wk9.html#reflection",
    "title": "7  SAR",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\n\n7.3.1 SAR Noise\nSAR differs from optical remote sensing in terms of imaging. Optical remote sensing is a form of incoherent imaging that only records the total energy of each pixel, because it relies on the reflection of natural light, which is incoherent and randomly phased.\nIn contrast, SAR is coherent imaging and produces more distinct interference patterns caused by the coherent summation of signals from many small scatterers, which is known as speckle noise. This type of noise affects the pixel values and texture information of objects, and is also unfavorable for visualization. Therefore, it needs to be processed in practical applications. Common methods include multi-looking and filtering. Both are essentially based on averaging: multi-looking averages observations from different viewing angles, while filtering averages values within a neighborhood window.\n\n\n7.3.2 SAR Bands\nAmong the various SAR bands, the C-band is currently the most widely used in research and operational applications. This is partly due to its data accessibility—for example, Sentinel-1 provides large volumes of open-access C-band data with high temporal frequency. Additionally, the C-band offers strong general-purpose performance, balancing penetration capability with spatial resolution.\nHowever, different SAR frequency bands exhibit different penetration depths and scattering characteristics, and each has its own advantages depending on the application. For example, in crop classification, the C-band is currently the most commonly used in agricultural monitoring. Yet, the S-band also shows great potential for such applications due to its better penetration and sensitivity to crop structural features. Its usage, however, is still limited by the relatively small number of available S-band sensors.\nLooking ahead, the development of diverse SAR sensors across multiple frequency bands, along with the launch of more SAR-equipped satellites, is expected to further expand the potential applications of SAR technology across a wide range of fields.\n\n\n\n\nAdugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of Random Forest and Support Vector Machine Classifiers for Regional Land Cover Mapping Using Coarse Resolution FY-3C Images.” Remote Sensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh, Meisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023. “Synthetic Aperture Radar (SAR) for Ocean: A Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005. “Desertification Alters Regional Ecosystem–Climate Interactions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma Kilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for Forest Variable Prediction in Boreal Region.” Remote Sensing of Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T. Calafate. 2022. “Flood Detection Using Real-Time Image Segmentation from Unmanned Aerial Vehicles on Edge-Computing Platform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image Enhancement Methods for Remote Sensing: A Survey.” In Recent Remote Sensing Sensor Applications, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based Classification of a Riparian Environment Using Ultra-High Resolution Imagery, Hierarchical Landcover Structures, and Image Texture.” Scientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for Computational Remote Sensing: Quantifying Patterns of Land Cover Types Around Cheetham Wetlands, Port Phillip Bay, Australia.” Journal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022. “Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM Texture Feature—a Case Study for Lousã Region, Portugal.” Remote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007. “Remote Sensing Applications: An Overview.” Current Science 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and Muhammad Shahbaz. 2025. “Enhancing Crop Classification Through Remote Sensing: Landsat 8-9 Fusion and Feature Selection.” Research Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using Google Earth Engine to Detect Land Cover Change: Singapore as a Use Case.” European Journal of Remote Sensing 51 (1): 486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications of Remote Sensing in Precision Agriculture: A Review.” Remote Sensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1 c-SAR VH/VV Polarization Data for Improved Crop Mapping.” Remote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie Fernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012. “Overview of Sentinel-2.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019. “Comparison of Maximum Likelihood, Support Vector Machines, and Random Forest Techniques in Satellite Images Classification.” Tecnura 23 (59): 13–26.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SAR</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Adugna, Tesfaye, Wenbo Xu, and Jinlong Fan. 2022. “Comparison of\nRandom Forest and Support Vector Machine Classifiers for Regional Land\nCover Mapping Using Coarse Resolution FY-3C Images.” Remote\nSensing 14 (3). https://doi.org/10.3390/rs14030574.\n\n\nAsiyabi, Reza Mohammadi, Arsalan Ghorbanian, Shaahin Nazarpour Tameh,\nMeisam Amani, Shuanggen Jin, and Ali Mohammadzadeh. 2023.\n“Synthetic Aperture Radar (SAR) for Ocean: A Review.”\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing 16: 9106–38. https://doi.org/10.1109/JSTARS.2023.3310363.\n\n\nAsner, Gregory P, and Kathleen B Heidebrecht. 2005.\n“Desertification Alters Regional Ecosystem–Climate\nInteractions.” Global Change Biology 11 (1): 182–94.\n\n\nAstola, Heikki, Tuomas Häme, Laura Sirro, Matthieu Molinier, and Jorma\nKilpi. 2019. “Comparison of Sentinel-2 and Landsat 8 Imagery for\nForest Variable Prediction in Boreal Region.” Remote Sensing\nof Environment 223: 257–73. https://doi.org/https://doi.org/10.1016/j.rse.2019.01.019.\n\n\nHernández, Daniel, José M. Cecilia, Juan-Carlos Cano, and Carlos T.\nCalafate. 2022. “Flood Detection Using Real-Time Image\nSegmentation from Unmanned Aerial Vehicles on Edge-Computing\nPlatform.” Remote Sensing 14 (1). https://doi.org/10.3390/rs14010223.\n\n\nKaplan, Nur Huseyin, Isin Erer, and Deniz Kumlu. 2021. “Image\nEnhancement Methods for Remote Sensing: A Survey.” In Recent\nRemote Sensing Sensor Applications, edited by Maged Marghany.\nRijeka: IntechOpen. https://doi.org/10.5772/intechopen.98527.\n\n\nKutz, Kain, Zachary Cook, and Marc Linderman. 2022. “Object Based\nClassification of a Riparian Environment Using Ultra-High Resolution\nImagery, Hierarchical Landcover Structures, and Image Texture.”\nScientific Reports 12 (1): 11291.\n\n\nLemenkova, Polina. 2024. “Artificial Intelligence for\nComputational Remote Sensing: Quantifying Patterns of Land Cover Types\nAround Cheetham Wetlands, Port Phillip Bay, Australia.”\nJournal of Marine Science and Engineering 12 (8): 10–3390.\n\n\nMohammadpour, Pegah, Domingos Xavier Viegas, and Carlos Viegas. 2022.\n“Vegetation Mapping with Random Forest Using Sentinel 2 and GLCM\nTexture Feature—a Case Study for Lousã Region, Portugal.”\nRemote Sensing 14 (18). https://doi.org/10.3390/rs14184585.\n\n\nNavalgund, Ranganath R., V. Jayaraman, and P. S. Roy. 2007.\n“Remote Sensing Applications: An Overview.” Current\nScience 93 (12): 1747–66. http://www.jstor.org/stable/24102069.\n\n\nRamzan, Zeeshan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, and\nMuhammad Shahbaz. 2025. “Enhancing Crop Classification Through\nRemote Sensing: Landsat 8-9 Fusion and Feature Selection.”\nResearch Square Preprint (Version 1). https://doi.org/10.21203/rs.3.rs-5710241/v1.\n\n\nSidhu, Nanki, Edzer Pebesma, and Gilberto Câmara. 2018. “Using\nGoogle Earth Engine to Detect Land Cover Change: Singapore as a Use\nCase.” European Journal of Remote Sensing 51 (1):\n486–500. https://doi.org/10.1080/22797254.2018.1451782.\n\n\nSishodia, R. P., R. L. Ray, and S. K. Singh. 2020. “Applications\nof Remote Sensing in Precision Agriculture: A Review.” Remote\nSensing 12: 3136. https://doi.org/10.3390/rs12193136.\n\n\nSonobe, Rei. 2019. “Combining ASNARO-2 XSAR HH and Sentinel-1\nc-SAR VH/VV Polarization Data for Improved Crop Mapping.”\nRemote Sensing 11 (16). https://doi.org/10.3390/rs11161920.\n\n\nSpoto, Francois, Omar Sy, Paolo Laberinti, Philippe Martimort, Valerie\nFernandez, Olivier Colin, Bianca Hoersch, and Aime Meygret. 2012.\n“Overview of Sentinel-2.” In 2012 IEEE International\nGeoscience and Remote Sensing Symposium, 1707–10. https://doi.org/10.1109/IGARSS.2012.6351195.\n\n\nValero Medina, José Antonio, and Beatriz Elena Alzate Atehortúa. 2019.\n“Comparison of Maximum Likelihood, Support Vector Machines, and\nRandom Forest Techniques in Satellite Images Classification.”\nTecnura 23 (59): 13–26.",
    "crumbs": [
      "References"
    ]
  }
]